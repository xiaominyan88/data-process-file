
<configuration>

	<property>
		<name>fs.AbstractFileSystem.alluxio.impl</name>
		<value>fs.AbstractFileSystem.alluxio.impl</value>
	</property>

	<property>
		<name>fs.alluxio-ft.impl</name>
		<value>alluxio.hadoop.FaultTolerantFileSystem</value>
	</property>

	<property>
		<name>fs.alluxio.impl</name>
		<value>alluxio.hadoop.FileSystem</value>
	</property>

	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://node3.eidp.bigdata.com:8020</value>
	</property>

	<property>
		<name>fs.trash.interval</name>
		<value>360</value>
	</property>

	<property>
		<name>ha.failover-controller.active-standby-elector.zk.op.retries</name>
		<value>120</value>
	</property>

	<property>
		<name>hadoop.http.authentication.center</name>
		<value>https://172.30.92.233:18443/sso</value>
	</property>

	<property>
		<name>hadoop.http.authentication.center.listener</name>
		<value>org.jasig.cas.client.session.SingleSignOutHttpSessionListener</value>
	</property>

	<property>
		<name>hadoop.http.authentication.simple.anonymous.allowed</name>
		<value>true</value>
	</property>

	<property>
		<name>hadoop.http.filter.initializers</name>
		<value>com.h3c.bigdata.auth.adapter.hadoop.FlowCtrlFilter,com.h3c.bigdata.auth.adapter.hadoop.AccessLogFilterInitializer,com.h3c.bigdata.auth.adapter.hadoop.InternalSpnegoFilter,com.h3c.bigdata.auth.adapter.hadoop.CASClientFilter</value>
	</property>

	<property>
		<name>hadoop.http.kerberos.internal.spnego.keytab</name>
		<value>/etc/security/keytabs/spnego.service.keytab</value>
	</property>

	<property>
		<name>hadoop.http.kerberos.internal.spnego.principal</name>
		<value>HTTP/_HOST@HDE.H3C.COM</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hbase.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hbase.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hcat.groups</name>
		<value>users</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hcat.hosts</name>
		<value>node4.eidp.bigdata.com</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hdfs.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hdfs.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hive.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hive.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.HTTP.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.HTTP.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hue.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.hue.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.root.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.root.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.spark.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.spark.hosts</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.yarn.groups</name>
		<value>*</value>
	</property>

	<property>
		<name>hadoop.proxyuser.yarn.hosts</name>
		<value>node3.eidp.bigdata.com</value>
	</property>

	<property>
		<name>hadoop.security.auth_to_local</name>
		<value>RULE:[1:$1@$0](ambari-qa-EIDP_BIGDATA@HDE.H3C.COM)s/.*/ambari-qa/
			RULE:[1:$1@$0](hbase-EIDP_BIGDATA@HDE.H3C.COM)s/.*/hbase/
			RULE:[1:$1@$0](hdfs-EIDP_BIGDATA@HDE.H3C.COM)s/.*/hdfs/
			RULE:[1:$1@$0](spark@HDE.H3C.COM)s/.*/spark/
			RULE:[1:$1@$0](.*@HDE.H3C.COM)s/@.*//
			RULE:[2:$1@$0](amshbase@HDE.H3C.COM)s/.*/ams/
			RULE:[2:$1@$0](amshbasemaster@HDE.H3C.COM)s/.*/ams/
			RULE:[2:$1@$0](amshbasers@HDE.H3C.COM)s/.*/ams/
			RULE:[2:$1@$0](amszk@HDE.H3C.COM)s/.*/ams/
			RULE:[2:$1@$0](dn@HDE.H3C.COM)s/.*/hdfs/
			RULE:[2:$1@$0](hbase@HDE.H3C.COM)s/.*/hbase/
			RULE:[2:$1@$0](hdfs@HDE.H3C.COM)s/.*/hdfs/
			RULE:[2:$1@$0](hdfs@HDE.H3C.COM)s/.*/spark/
			RULE:[2:$1@$0](jhs@HDE.H3C.COM)s/.*/mapred/
			RULE:[2:$1@$0](jn@HDE.H3C.COM)s/.*/hdfs/
			RULE:[2:$1@$0](nfs@HDE.H3C.COM)s/.*/hdfs/
			RULE:[2:$1@$0](nm@HDE.H3C.COM)s/.*/yarn/
			RULE:[2:$1@$0](nn@HDE.H3C.COM)s/.*/hdfs/
			RULE:[2:$1@$0](rm@HDE.H3C.COM)s/.*/yarn/
			RULE:[2:$1@$0](yarn@HDE.H3C.COM)s/.*/yarn/
			DEFAULT</value>
	</property>

	<property>
		<name>hadoop.security.authentication</name>
		<value>kerberos</value>
	</property>

	<property>
		<name>hadoop.security.authorization</name>
		<value>true</value>
	</property>

	<property>
		<name>hadoop.security.key.provider.path</name>
		<value></value>
	</property>

	<property>
		<name>io.compression.codecs</name>
		<value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
	</property>

	<property>
		<name>io.file.buffer.size</name>
		<value>131072</value>
	</property>

	<property>
		<name>io.serializations</name>
		<value>org.apache.hadoop.io.serializer.WritableSerialization</value>
	</property>

	<property>
		<name>ipc.client.connect.max.retries</name>
		<value>50</value>
	</property>

	<property>
		<name>ipc.client.connection.maxidletime</name>
		<value>30000</value>
	</property>

	<property>
		<name>ipc.client.fallback-to-simple-auth-allowed</name>
		<value>true</value>
	</property>

	<property>
		<name>ipc.client.idlethreshold</name>
		<value>8000</value>
	</property>

	<property>
		<name>ipc.server.tcpnodelay</name>
		<value>true</value>
	</property>

	<property>
		<name>mapreduce.jobtracker.webinterface.trusted</name>
		<value>false</value>
	</property>

	<property>
		<name>net.topology.script.file.name</name>
		<value>/etc/hadoop/conf/topology_script.py</value>
	</property>

</configuration>